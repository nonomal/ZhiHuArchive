<!DOCTYPE html>
<html lang="zh">
<head>
<title>商汤发布首个「可控」人物视频生成大模型 Vimi，Vimi 大模型应用了哪些关键性技术？ - @叶峻峣</title>
<meta charset="UTF-8">
<meta property="og:type" content="website">
<meta property="og:title" content="商汤发布首个「可控」人物视频生成大模型 Vimi，Vimi 大模型应用了哪些关键性技术？ - @叶峻峣">
<meta property="og:site_name" content="ZhiHu Archive for Thoughts Memo">
<meta property="og:url" content="https://www.zhihu.com/question/660746007/answer/3555321700">
<meta name="description" property="og:description" content="首个？我同学上个月就开源了他们的可控视频生成模型 ControlNeXt：dvlab-research/ControlNeXt: Controllable video and image Generation, SVD, Animate Anyone, ControlNet, LoRA (github.com) [视频] [视频] 这下又新闻学魅力时刻了。 具体的技术介绍还在待发表的论文中，敬请期待。">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/yue.css@0.4.0/yue.css">
<meta property="twitter:card" content="summary">
<meta name="twitter:title" property="og:title" itemprop="name" content="商汤发布首个「可控」人物视频生成大模型 Vimi，Vimi 大模型应用了哪些关键性技术？ - @叶峻峣">
<meta name="twitter:description" property="og:description" itemprop="description" content="首个？我同学上个月就开源了他们的可控视频生成模型 ControlNeXt：dvlab-research/ControlNeXt: Controllable video and image Generation, SVD, Animate Anyone, ControlNet, LoRA (github.com) [视频] [视频] 这下又新闻学魅力时刻了。 具体的技术介绍还在待发表的论文中，敬请期待。">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">
<meta name="google-site-verification" content="U7ZAFUgGNK60mmMqaRygg5vy-k8pwbPbDFXNjDCu7Xk" />
<link rel="alternate" type="application/rss+xml" title="ZhiHu Archive for Thoughts Memo" href="https://l-m-sherlock.github.io/ZhiHuArchive/feed.xml">
<script>
</script>
<style>
img {
vertical-align: middle;
}
figure img {
width: 100%;
}
figure {
margin: 1.4em 0;
}
.author {
display: flex;
gap: 1em;
}
#avatar {
width: 100px;
height: 100px;
}
.author > div {
flex: 1;
}
a[data-draft-type="link-card"] {
   display: block;
}
</style>
</head>
<body style="max-width: 1000px; margin: 0 auto; padding: 0 1em 0 1em;" class="yue">
<header>
<h1><a href="https://www.zhihu.com/question/660746007/answer/3555321700">商汤发布首个「可控」人物视频生成大模型 Vimi，Vimi 大模型应用了哪些关键性技术？</a></h1>
<div class="author">
<img class="avatar" id="avatar" src="https://picx.zhimg.com/v2-0ebbd00d10ee9f3b47237b367659abe0_l.jpg?source=2c26e567" />
<div>
<h2 rel="author">
<a href="https://api.zhihu.com/people/3c9990a12cdbcd92e20b1387b160f0a3" target="_blank">@叶峻峣</a>
</h2>
<p> 钻研人类记忆，探索复习算法。改善教育公平，践行自由学习。 </p>
</div>
</div>
<time datetime="2024-07-08T07:51:30">发表于 2024年07月08日</time>
<p rel="stats"style="color: #999; font-size: 0.9em;">23 👍 / 2 💬</p>
</header>
<article>
<div style="margin: 0; padding: 0.5em 1em; border-left: 4px solid #999; font-size: 0.86em; background: #f9f9f9;">
<h2>问题描述</h2>
<p>7月4日，界面新闻获悉，商汤发布首个“可控”人物视频生成大模型Vimi，该模型主要面向C端用户，支持聊天、唱歌、舞动等多种娱乐互动场景。商汤方面称，Vimi可生成长达1分钟的单镜头人物类视频，画面效果不会随着时间的变化而劣化或失真，Vimi基于商汤日日新大模型，通过一张任意风格的照片就能生成和目标动作一致的人物类视频，可通过已有人物视频、动画、声音、文字等多种元素进行驱动。</p><a href="https://link.zhihu.com/?target=https%3A//finance.sina.com.cn/jjxw/2024-07-04/doc-incaxsxm5986626.shtml" data-draft-node="block" data-draft-type="link-card" data-image="" data-image-width="" data-image-height="" class=" wrap external" target="_blank" rel="nofollow noreferrer">商汤发布首个“可控”人物视频生成大模型Vimi</a>
</div>
<hr>
<p data-pid="A7aV-si7">首个？我同学上个月就开源了他们的可控视频生成模型 <span class="nolink">ControlNeXt</span>：</p><a href="https://github.com/dvlab-research/ControlNeXt" data-draft-node="block" data-draft-type="link-card" class=" wrap external" target="_blank" rel="nofollow noreferrer">dvlab-research/ControlNeXt: Controllable video and image Generation, SVD, Animate Anyone, ControlNet, LoRA (github.com)</a><a class="video-box" href="https://www.zhihu.com/video/1793672973791678464" target="_blank" data-video-id="" data-video-playable="true" data-name="" data-poster="https://picx.zhimg.com/v2-fd4ac111b203c7e2580fff67554048f7.jpg?source=382ee89a" data-lens-id="1793672973791678464"><img class="thumbnail" src="https://picx.zhimg.com/v2-fd4ac111b203c7e2580fff67554048f7.jpg?source=382ee89a"/><span class="content"><span class="title"><span class="z-ico-extern-gray"></span><span class="z-ico-extern-blue"></span></span><span class="url"><span class="z-ico-video"></span>https://www.zhihu.com/video/1793672973791678464</span></span></a><a class="video-box" href="https://www.zhihu.com/video/1793673014547705859" target="_blank" data-video-id="" data-video-playable="true" data-name="" data-poster="https://picx.zhimg.com/v2-8ef520ef6b743daa8aa8a4c2c04629c8.jpg?source=382ee89a" data-lens-id="1793673014547705859"><img class="thumbnail" src="https://picx.zhimg.com/v2-8ef520ef6b743daa8aa8a4c2c04629c8.jpg?source=382ee89a"/><span class="content"><span class="title"><span class="z-ico-extern-gray"></span><span class="z-ico-extern-blue"></span></span><span class="url"><span class="z-ico-video"></span>https://www.zhihu.com/video/1793673014547705859</span></span></a><p data-pid="wf6xP71T">这下又新闻学魅力时刻了。</p><p data-pid="AfgBOAG0">具体的技术介绍还在待发表的论文中，敬请期待。</p>

<hr>
<p><a href="./">← 返回目录</a></p>
</article>
</body>
</html>