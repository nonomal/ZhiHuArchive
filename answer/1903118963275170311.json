{
  "answer_type": "normal",
  "author": {
    "avatar_url": "https://picx.zhimg.com/v2-b10214702e40f6088c1f71cc1c60ca83_l.jpg?source=2c26e567",
    "avatar_url_template": "https://pic1.zhimg.com/v2-b10214702e40f6088c1f71cc1c60ca83.jpg?source=2c26e567",
    "badge": [
      {
        "description": "信息技术行业 算法工程师",
        "topics": [],
        "type": "identity"
      }
    ],
    "badge_v2": {
      "detail_badges": [
        {
          "badge_status": "passed",
          "description": "知势榜教育校园领域影响力榜答主",
          "detail_type": "super_activity",
          "icon": "https://pic1.zhimg.com/v2-4a07bc69c4bb04444721f35b32125c75_l.png?source=32738c0c",
          "night_icon": "https://picx.zhimg.com/v2-4a07bc69c4bb04444721f35b32125c75_l.png?source=32738c0c",
          "sources": [
            {
              "avatar_path": "",
              "avatar_url": "",
              "description": "",
              "id": "27",
              "name": "知势榜8月",
              "priority": 27,
              "token": "",
              "type": "content_potential_category",
              "url": ""
            }
          ],
          "title": "社区成就",
          "type": "reward",
          "url": ""
        },
        {
          "badge_status": "passed",
          "description": "信息技术行业 算法工程师",
          "detail_type": "identity_people",
          "icon": "https://pica.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c",
          "night_icon": "https://picx.zhimg.com/v2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c",
          "sources": [],
          "title": "已认证的个人",
          "type": "identity",
          "url": "https://zhuanlan.zhihu.com/p/96956163"
        }
      ],
      "icon": "https://picx.zhimg.com/v2-27bfcba90e66db79ce8768ab807e017e_l.png?source=32738c0c",
      "merged_badges": [
        {
          "badge_status": "passed",
          "description": "知势榜教育校园领域影响力榜答主",
          "detail_type": "best",
          "icon": "",
          "night_icon": "",
          "sources": [],
          "title": "社区成就",
          "type": "best",
          "url": ""
        },
        {
          "badge_status": "passed",
          "description": "信息技术行业 算法工程师",
          "detail_type": "identity_people",
          "icon": "",
          "night_icon": "",
          "sources": [],
          "title": "认证",
          "type": "identity",
          "url": "https://zhuanlan.zhihu.com/p/96956163"
        }
      ],
      "night_icon": "https://picx.zhimg.com/v2-27bfcba90e66db79ce8768ab807e017e_l.png?source=32738c0c",
      "title": "知势榜教育校园领域影响力榜答主"
    },
    "gender": -1,
    "headline": "学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本",
    "id": "4c592f496dc33822b560b382907ff1d0",
    "is_advertiser": false,
    "is_org": false,
    "is_privacy": false,
    "name": "Thoughts Memo",
    "type": "people",
    "url": "https://api.zhihu.com/people/4c592f496dc33822b560b382907ff1d0",
    "url_token": "L.M.Sherlock",
    "user_type": "people"
  },
  "biz_ext": {
    "share_guide": {
      "has_positive_bubble": false,
      "has_time_bubble": false,
      "hit_share_guide_cluster": false
    }
  },
  "comment_count": 8,
  "content": "<p data-pid=\"wsNrllXB\">求求你们来做点新数据集、新任务上的工作吧！（顺带 sell 一下我的 work）</p><h2>在间隔重复领域对学生记忆的时间序列预测任务</h2><p data-pid=\"MGVm9g5G\">项目地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-spaced-repetition/srs-benchmark\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">open-spaced-repetition/srs-benchmark: A benchmark for spaced repetition schedulers/algorithms</a></p><h2><b>引言</b></h2><p data-pid=\"tL1NhHXp\">间隔重复算法（Spaced Repetition Algorithms）是一种计算机程序，旨在帮助人们规划抽认卡（flashcards）的复习安排。优秀的间隔重复算法能助你更高效地记忆。它并非让用户进行一次性的突击记忆（cramming），而是将复习活动分散到不同的时间点。为了实现高效复习，这些算法试图理解人类记忆的运作机制，旨在预测你可能遗忘某个知识点的时间点，以便据此安排下一次复习。</p><p data-pid=\"UkxMPGQH\">本基准测试旨在评估各种算法的预测准确性。我们评估了多种算法，以期找出能提供最准确预测的算法。</p><h2><b>数据集</b></h2><p data-pid=\"b09W1jdh\">SRS 基准的数据集来自 1 万名使用 Anki（一个开源的抽认卡应用）的用户。该数据集包含约 7.27 亿次复习记录。完整数据集托管在 Hugging Face Datasets 上：</p><a href=\"https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/open-spaced-repetition/anki-revlogs-10k\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">open-spaced-repetition/anki-revlogs-10k · Datasets at Hugging Face</a><h2><b>评估</b></h2><h3><b>数据划分</b></h3><p data-pid=\"tYPS_XuK\">在间隔重复系统（SRS）基准测试中，我们使用了一个名为 <code>TimeSeriesSplit</code> 的工具，它是机器学习库 <a href=\"https://link.zhihu.com/?target=https%3A//scikit-learn.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn</a> 的一部分。该工具帮助我们按时间顺序划分数据：较早的复习记录用于训练模型，较新的复习记录用于测试模型。这样，我们就能避免通过让算法获取本不应得到的未来信息而「作弊」。实践中，我们正是利用过去的的学习记录来预测未来的学习表现，因此 <code>TimeSeriesSplit</code> 非常适合我们的基准测试需求。</p><p data-pid=\"qO1VsbqB\">注意：<code>TimeSeriesSplit</code> 在评估时会排除第一个数据分割。这是因为第一个分割用于训练，我们不希望在训练数据上评估模型的性能。</p><h3><b>评估指标</b></h3><p data-pid=\"FLQE1nf8\">我们在 SRS 基准测试中使用三个指标来评估算法性能：对数损失（Log Loss）、曲线下面积（AUC）以及一个我们称为「分箱均方根误差」（RMSE (bins)）的自定义指标。</p><ul><li data-pid=\"K5kuc8Gk\"><b>对数损失 (Log Loss)</b>（亦称二元交叉熵）：主要用于二元分类问题，对数损失衡量的是预测回忆概率与实际复习结果（1 或 0）之间的偏差。它量化了算法对真实回忆概率的拟合优度。对数损失的取值范围为 0 到无穷大，值越低表示性能越好。<br/><br/></li><li data-pid=\"PWvD0fje\"><b>分箱均方根误差 (RMSE (bins))</b>：这是专为 SRS 基准测试设计的指标。该方法将预测结果和实际复习结果依据三个特征（复习间隔时长、复习次数、（记忆）中断次数）划分到不同的「箱」中。在每个箱内，计算平均预测回忆概率与平均实际回忆率之间的平方差。然后，根据每个箱内的样本数量对这些平方差进行加权，最终计算出加权均方根误差。该指标能够提供对算法在不同概率区间性能表现的更细致的理解。更多详情，请参阅<a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-spaced-repetition/fsrs4anki/wiki/The-Metric\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">该指标说明</a>。RMSE (bins) 的取值范围为 0 到 1，值越低表示性能越好。<br/><br/></li><li data-pid=\"XmcOcKf-\"><b>AUC (ROC 曲线下面积)</b>：该指标衡量算法区分不同类别（在此场景下指回忆成功与失败）的能力。AUC 的取值范围为 0 到 1，但在实践中几乎总是大于 0.5；值越高表示性能越好。</li></ul><p data-pid=\"tfbBS-fE\">对数损失和 RMSE (bins) 主要衡量<b>校准度 (calibration)</b>：即预测的回忆概率与实际观测数据的一致性。AUC 主要衡量<b>区分度 (discrimination)</b>：即算法区分两个（或更广义地说，多个）类别的能力。即使对数损失和 RMSE 指标表现不佳，AUC 也可能很高。</p><h3><b>算法及算法家族</b></h3><ul><li data-pid=\"rwb4dTMR\"><b>双组件或三组件*记忆模型</b>：<br/></li><ul><li data-pid=\"eyPjvk3y\">FSRS v1 和 v2：FSRS 的早期实验版本。</li><li data-pid=\"3FFcv1-c\">FSRS v3：FSRS 算法的首个正式发布版本，以自定义调度脚本的形式提供。</li><li data-pid=\"RdFfHOrz\">FSRS v4：FSRS 的升级版，在社区的帮助下进行了改进。</li><li data-pid=\"T70C5wum\">FSRS-4.5：基于 FSRS v4 的微幅改进版，主要调整了遗忘曲线的形状。</li><li data-pid=\"5Z80-zxu\">FSRS-5：FSRS 的升级版。与先前版本不同，它利用了当日复习数据。当日复习数据仅用于训练，不用于评估。</li><li data-pid=\"FhZdk-A_\">FSRS-6：FSRS 的最新版本。改进了处理当日复习数据的公式。更重要的是，FSRS-6 引入了一个可优化的参数来控制遗忘曲线的平坦程度，这意味着不同用户的遗忘曲线形状可以不同。</li><ul><li data-pid=\"7RWMBloI\">FSRS-6 default param.：使用默认参数的 FSRS-6。这些默认参数是通过在数据集中全部一万个用户数据集上运行 FSRS-6 并计算每个参数的中位数得到的。</li><li data-pid=\"NV_5RgHM\">FSRS-6 pretrain：仅优化前 4 个参数（首次复习后的初始稳定性值），其余参数设为默认值的 FSRS-6。</li><li data-pid=\"1vlZu-9s\">FSRS-6 binary：将「困难」和「简单」两种评级均视为「良好」的 FSRS-6。</li><li data-pid=\"BC3n-NII\">FSRS-6 preset：每个预设配置使用不同的参数。在 Anki 中，最少可以有一个预设配置，一个预设可以应用于多个牌组。</li><li data-pid=\"z4ZKHyh-\">FSRS-6 deck：每个牌组使用不同的参数。</li><li data-pid=\"eYWt1ek1\">FSRS-6 recency：训练时根据复习记录的新近度进行加权的 FSRS-6，使得较早的复习记录对损失函数的影响更小，较新的记录影响更大。</li></ul><li data-pid=\"6GIe5j-a\">FSRS-rs：FSRS-6 的 Rust 语言移植版。另见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-spaced-repetition/fsrs-rs\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/open-spaced-</span><span class=\"invisible\">repetition/fsrs-rs</span><span class=\"ellipsis\"></span></a></li><li data-pid=\"HwNJVRHg\">HLR：由 Duolingo 提出的算法，全称为半衰期回归（Half-Life Regression）。更多信息请参阅<a href=\"https://link.zhihu.com/?target=https%3A//github.com/duolingo/halflife-regression\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这篇论文</a>。</li><li data-pid=\"Lga3lB3D\">Ebisu v2：<a href=\"https://link.zhihu.com/?target=https%3A//fasiha.github.io/ebisu/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一种利用贝叶斯统计</a>在每次复习后更新记忆半衰期估计值的算法。</li></ul></ul><p data-pid=\"5G6fddTi\">*在长期记忆的双组件模型中，使用两个独立变量描述人脑中单个记忆项目的状态：<b>可提取性 (R)</b>，即检索强度或回忆概率；以及<b>稳定性 (S)</b>，即存储强度或记忆半衰期。扩展的三组件模型增加了第三个变量——<b>难度 (D)</b>。</p><ul><li data-pid=\"vt_7L6Um\"><b>其他记忆模型</b>：</li><ul><li data-pid=\"LV-kbKIT\">DASH：在<a href=\"https://link.zhihu.com/?target=https%3A//scholar.colorado.edu/concern/graduate_thesis_or_dissertations/zp38wc97m\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这篇论文</a>中提出的算法，其名称代表难度（Difficulty）、能力（Ability）和学习历史（Study History）。在我们的基准测试中，由于难度部分不适用于我们的数据集，我们仅使用了能力和学习历史部分。我们还增加了该算法的两个变体：DASH[MCM] 和 DASH[ACT-R]。更多信息请参阅<a href=\"https://link.zhihu.com/?target=https%3A//www.politesi.polimi.it/retrieve/b39227dd-0963-40f2-a44b-624f205cb224/2022_4_Randazzo_01.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这篇论文</a>。</li><li data-pid=\"FUB8mn_j\">ACT-R：在<a href=\"https://link.zhihu.com/?target=http%3A//act-r.psy.cmu.edu/wordpress/wp-content/themes/ACT-R/workshops/2003/proceedings/46.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这篇论文</a>中提出的算法。它包含一个基于激活的陈述性记忆系统，通过记忆痕迹的激活来解释间隔效应。</li></ul><li data-pid=\"V18A0pmd\"><b>神经网络模型</b>：</li><ul><li data-pid=\"artJsq6i\">GRU：一种循环神经网络，常用于基于数据序列进行预测。它是机器学习领域处理时间序列相关任务的经典模型之一。为使比较更公平，它采用了与 FSRS-4.5 和 FSRS-5 相同的幂律遗忘曲线。</li><ul><li data-pid=\"X0-WPvei\">GRU-P：GRU 的一个变体，移除了固定的遗忘曲线，直接预测回忆概率。这使其比 GRU 更灵活，但也更容易产生奇怪的预测，例如预测回忆概率随时间<b>增加</b>。</li></ul><li data-pid=\"xBKRFOt9\">LSTM：一种比 GRU 架构更复杂、更精密的循环神经网络。它使用 <a href=\"https://link.zhihu.com/?target=https%3A//openai.com/index/reptile/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Reptile 算法</a>进行训练，并将短期复习记录、小数间隔以及复习时长作为其输入的一部分。 上述三种神经网络首先在 100 个用户的数据上进行了预训练，然后针对每个用户的数据单独进行了进一步优化。</li><li data-pid=\"M7wB-ALE\">NN-17：<a href=\"https://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SM-17</a> 算法的神经网络近似。它具有数量相当的参数，根据我们的估计，其性能与 SM-17 相近。</li></ul><li data-pid=\"YUmFERqq\"><b>基于 SM-2 的算法</b>：</li><ul><li data-pid=\"183ndxBo\">SM-2：SuperMemo（首款间隔重复软件）早期使用的算法之一。它诞生于 30 多年前，至今仍广受欢迎。<a href=\"https://link.zhihu.com/?target=https%3A//faqs.ankiweb.net/what-spaced-repetition-algorithm.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Anki 的默认算法基于 SM-2</a>，<a href=\"https://link.zhihu.com/?target=https%3A//mnemosyne-proj.org/principles.php\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Mnemosyne</a> 也在使用它。该算法本身并不直接预测回忆概率；因此，为了进行基准测试，我们基于一些关于遗忘曲线的假设对其输出进行了修改。Piotr Wozniak <a href=\"https://link.zhihu.com/?target=https%3A//super-memory.com/english/ol/sm2.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">在此</a>处描述了该算法。</li><ul><li data-pid=\"WKprVsrw\">SM-2 trainable：具有可优化参数的 SM-2 算法。</li></ul><li data-pid=\"1tZ1jnQq\">Anki-SM-2：Anki 中使用的 SM-2 算法变体。</li><ul><li data-pid=\"zC8FLJbV\">Anki-SM-2 trainable：具有可优化参数的 Anki 算法。</li></ul></ul><li data-pid=\"rcALTATc\"><b>其他</b>：</li><ul><li data-pid=\"xyWAPQE5\">AVG：一个输出恒定值（等于用户平均记忆保持率）的「算法」。没有实际应用价值，仅用作性能基线。</li><li data-pid=\"Rm1ft7DC\">RMSE-BINS-EXPLOIT：一种利用 RMSE (bins) 计算方式的算法，通过模拟分箱操作将误差项维持在接近 0 的水平。</li></ul></ul><p data-pid=\"mV2laafY\">关于 FSRS 算法的进一步信息，请参阅以下维基页面：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-spaced-repetition/fsrs4anki/wiki/The-Algorithm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">该算法</a>。</p><h2><b>结果</b></h2><p data-pid=\"TdofRF1g\">总用户数：9,999。</p><p data-pid=\"YOniMNHS\">用于评估的总复习次数：349,923,850。 当日复习数据不用于评估，但部分算法会利用这些数据来优化对次日回忆概率的预测。部分复习记录会被过滤掉，例如：手动更改到期日所产生的复习日志条目，或在禁用「根据我在此牌组中的答案重新安排卡片」选项的筛选牌组中复习卡片所产生的记录。最后，还会应用异常值过滤器。这些是导致用于评估的复习次数远低于之前提到的 7.27 亿的原因。</p><p data-pid=\"POYLGB7f\">下表展示了各项指标的均值和 99% 置信区间。最佳结果以<b>粗体</b>标出。「参数」列显示了可优化（可训练）参数的数量，恒定参数不计入。箭头指示指标值越低 (↓) 或越高 (↑) 越好。</p><p data-pid=\"yddWf85h\">为简洁起见，「输入特征」列中使用了以下缩写：</p><p data-pid=\"Shm4vore\"><b>IL</b> = <b>i</b>nterval <b>l</b>engths, in days</p><p data-pid=\"Vdg4FAQ2\"><b>FIL</b> = <b>f</b>ractional (aka non-integer) <b>i</b>nterval <b>l</b>engths</p><p data-pid=\"7XAvi3vi\"><b>G</b> = <b>g</b>rades (Again/Hard/Good/Easy)</p><p data-pid=\"jYZiRJyc\"><b>SR</b> = <b>s</b>ame-day (or <b>s</b>hort-term) <b>r</b>eviews</p><p data-pid=\"ojgcKXXD\"><b>AT</b> = <b>a</b>nswer <b>t</b>ime (duration of the review), in milliseconds</p><h3>根据复习数量加权</h3><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><th>Algorithm</th><th>Parameters</th><th>Log Loss↓</th><th>RMSE (bins)↓</th><th>AUC↑</th><th>Input features</th></tr><tr><td>LSTM</td><td>8869</td><td>0.312±0.0078</td><td>0.035±0.0011</td><td>0.733±0.0038</td><td>FIL, G, SR, AT</td></tr><tr><td>GRU-P-short</td><td>297</td><td>0.320±0.0080</td><td>0.042±0.0013</td><td>0.710±0.0047</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 recency</td><td>21</td><td>0.320±0.0081</td><td>0.044±0.0013</td><td>0.710±0.0040</td><td>IL, G, SR</td></tr><tr><td>FSRS-rs</td><td>21</td><td>0.320±0.0082</td><td>0.044±0.0012</td><td>0.709±0.0041</td><td>IL, G, SR</td></tr><tr><td>FSRS-6</td><td>21</td><td>0.321±0.0083</td><td>0.046±0.0013</td><td>0.706±0.0041</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 preset</td><td>21</td><td>0.322±0.0081</td><td>0.046±0.0013</td><td>0.707±0.0041</td><td>IL, G, SR</td></tr><tr><td>GRU-P</td><td>297</td><td>0.325±0.0081</td><td>0.043±0.0013</td><td>0.699±0.0046</td><td>IL, G</td></tr><tr><td>FSRS-6 binary</td><td>17</td><td>0.326±0.0081</td><td>0.049±0.0013</td><td>0.686±0.0047</td><td>IL, G, SR</td></tr><tr><td>FSRS-5</td><td>19</td><td>0.327±0.0083</td><td>0.052±0.0015</td><td>0.702±0.0042</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 deck</td><td>21</td><td>0.329±0.0082</td><td>0.052±0.0016</td><td>0.699±0.0041</td><td>IL, G, SR</td></tr><tr><td>FSRS-4.5</td><td>17</td><td>0.332±0.0083</td><td>0.054±0.0016</td><td>0.692±0.0041</td><td>IL, G</td></tr><tr><td>FSRS v4</td><td>17</td><td>0.338±0.0086</td><td>0.058±0.0017</td><td>0.689±0.0043</td><td>IL, G</td></tr><tr><td>DASH-short</td><td>9</td><td>0.339±0.0084</td><td>0.066±0.0019</td><td>0.636±0.0050</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 pretrain</td><td>4</td><td>0.339±0.0084</td><td>0.070±0.0024</td><td>0.695±0.0039</td><td>IL, G, SR</td></tr><tr><td>DASH</td><td>9</td><td>0.340±0.0086</td><td>0.063±0.0017</td><td>0.639±0.0046</td><td>IL, G</td></tr><tr><td>DASH[MCM]</td><td>9</td><td>0.340±0.0085</td><td>0.064±0.0018</td><td>0.640±0.0051</td><td>IL, G</td></tr><tr><td>GRU</td><td>39</td><td>0.343±0.0088</td><td>0.063±0.0017</td><td>0.673±0.0039</td><td>IL, G</td></tr><tr><td>DASH[ACT-R]</td><td>5</td><td>0.343±0.0087</td><td>0.067±0.0019</td><td>0.629±0.0049</td><td>IL, G</td></tr><tr><td>FSRS-6 default param.</td><td>0</td><td>0.347±0.0087</td><td>0.079±0.0027</td><td>0.692±0.0041</td><td>IL, G, SR</td></tr><tr><td>ACT-R</td><td>5</td><td>0.362±0.0089</td><td>0.086±0.0024</td><td>0.534±0.0054</td><td>IL</td></tr><tr><td>AVG</td><td>0</td><td>0.363±0.0090</td><td>0.088±0.0025</td><td>0.508±0.0046</td><td>---</td></tr><tr><td>FSRS v3</td><td>13</td><td>0.371±0.0099</td><td>0.073±0.0021</td><td>0.667±0.0047</td><td>IL, G</td></tr><tr><td>FSRS v2</td><td>14</td><td>0.38±0.010</td><td>0.069±0.0021</td><td>0.667±0.0048</td><td>IL, G</td></tr><tr><td>NN-17</td><td>39</td><td>0.38±0.027</td><td>0.081±0.0038</td><td>0.611±0.0043</td><td>IL, G</td></tr><tr><td>FSRS v1</td><td>7</td><td>0.40±0.011</td><td>0.086±0.0024</td><td>0.633±0.0046</td><td>IL, G</td></tr><tr><td>Anki-SM-2 trainable</td><td>7</td><td>0.41±0.011</td><td>0.094±0.0030</td><td>0.616±0.0057</td><td>IL, G</td></tr><tr><td>HLR</td><td>3</td><td>0.41±0.012</td><td>0.105±0.0030</td><td>0.633±0.0050</td><td>IL, G</td></tr><tr><td>HLR-short</td><td>3</td><td>0.44±0.013</td><td>0.116±0.0036</td><td>0.615±0.0062</td><td>IL, G, SR</td></tr><tr><td>SM-2 trainable</td><td>6</td><td>0.44±0.012</td><td>0.119±0.0033</td><td>0.599±0.0050</td><td>IL, G</td></tr><tr><td>Ebisu v2</td><td>0</td><td>0.46±0.012</td><td>0.158±0.0038</td><td>0.594±0.0050</td><td>IL, G</td></tr><tr><td>Anki-SM-2</td><td>0</td><td>0.49±0.015</td><td>0.128±0.0037</td><td>0.597±0.0055</td><td>IL, G</td></tr><tr><td>SM-2-short</td><td>0</td><td>0.51±0.015</td><td>0.128±0.0038</td><td>0.593±0.0064</td><td>IL, G, SR</td></tr><tr><td>SM-2</td><td>0</td><td>0.55±0.017</td><td>0.148±0.0041</td><td>0.600±0.0051</td><td>IL, G</td></tr><tr><td>RMSE-BINS-EXPLOIT</td><td>0</td><td>4.5±0.13</td><td>0.0062±0.00022</td><td>0.638±0.0040</td><td>IL, G</td></tr></tbody></table><h3>未加权</h3><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><th>Algorithm</th><th>Parameters</th><th>Log Loss↓</th><th>RMSE (bins)↓</th><th>AUC↑</th><th>Input features</th></tr><tr><td>LSTM</td><td>8869</td><td>0.333±0.0042</td><td>0.0538±0.00096</td><td>0.733±0.0021</td><td>FIL, G, SR, AT</td></tr><tr><td>FSRS-6 recency</td><td>21</td><td>0.344±0.0041</td><td>0.063±0.0010</td><td>0.710±0.0023</td><td>IL, G, SR</td></tr><tr><td>FSRS-rs</td><td>21</td><td>0.344±0.0041</td><td>0.063±0.0010</td><td>0.710±0.0022</td><td>IL, G, SR</td></tr><tr><td>FSRS-6</td><td>21</td><td>0.345±0.0042</td><td>0.066±0.0011</td><td>0.707±0.0023</td><td>IL, G, SR</td></tr><tr><td>GRU-P-short</td><td>297</td><td>0.346±0.0042</td><td>0.062±0.0011</td><td>0.699±0.0026</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 preset</td><td>21</td><td>0.346±0.0042</td><td>0.065±0.0010</td><td>0.708±0.0023</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 binary</td><td>17</td><td>0.351±0.0043</td><td>0.068±0.0011</td><td>0.685±0.0025</td><td>IL, G, SR</td></tr><tr><td>GRU-P</td><td>297</td><td>0.352±0.0042</td><td>0.063±0.0011</td><td>0.687±0.0025</td><td>IL, G</td></tr><tr><td>FSRS-6 deck</td><td>21</td><td>0.355±0.0045</td><td>0.074±0.0013</td><td>0.703±0.0023</td><td>IL, G, SR</td></tr><tr><td>FSRS-5</td><td>19</td><td>0.356±0.0043</td><td>0.074±0.0012</td><td>0.701±0.0023</td><td>IL, G, SR</td></tr><tr><td>FSRS-6 pretrain</td><td>4</td><td>0.359±0.0044</td><td>0.083±0.0013</td><td>0.702±0.0022</td><td>IL, G, SR</td></tr><tr><td>FSRS-4.5</td><td>17</td><td>0.362±0.0045</td><td>0.076±0.0013</td><td>0.689±0.0023</td><td>IL, G</td></tr><tr><td>DASH</td><td>9</td><td>0.368±0.0045</td><td>0.084±0.0013</td><td>0.631±0.0027</td><td>IL, G</td></tr><tr><td>DASH-short</td><td>9</td><td>0.368±0.0045</td><td>0.086±0.0014</td><td>0.622±0.0029</td><td>IL, G, SR</td></tr><tr><td>DASH[MCM]</td><td>9</td><td>0.369±0.0044</td><td>0.086±0.0014</td><td>0.634±0.0026</td><td>IL, G</td></tr><tr><td>FSRS-6 default param.</td><td>0</td><td>0.371±0.0046</td><td>0.097±0.0015</td><td>0.701±0.0022</td><td>IL, G, SR</td></tr><tr><td>FSRS v4</td><td>17</td><td>0.373±0.0048</td><td>0.084±0.0014</td><td>0.685±0.0023</td><td>IL, G</td></tr><tr><td>DASH[ACT-R]</td><td>5</td><td>0.373±0.0047</td><td>0.089±0.0016</td><td>0.624±0.0027</td><td>IL, G</td></tr><tr><td>GRU</td><td>39</td><td>0.375±0.0047</td><td>0.086±0.0014</td><td>0.668±0.0023</td><td>IL, G</td></tr><tr><td>AVG</td><td>0</td><td>0.394±0.0050</td><td>0.103±0.0016</td><td>0.500±0.0026</td><td>---</td></tr><tr><td>NN-17</td><td>39</td><td>0.398±0.0049</td><td>0.101±0.0013</td><td>0.624±0.0023</td><td>IL, G</td></tr><tr><td>ACT-R</td><td>5</td><td>0.403±0.0055</td><td>0.107±0.0017</td><td>0.522±0.0024</td><td>IL</td></tr><tr><td>FSRS v3</td><td>13</td><td>0.436±0.0067</td><td>0.110±0.0020</td><td>0.661±0.0024</td><td>IL, G</td></tr><tr><td>FSRS v2</td><td>14</td><td>0.453±0.0072</td><td>0.110±0.0020</td><td>0.651±0.0023</td><td>IL, G</td></tr><tr><td>HLR</td><td>3</td><td>0.469±0.0073</td><td>0.128±0.0019</td><td>0.637±0.0026</td><td>IL, G</td></tr><tr><td>FSRS v1</td><td>7</td><td>0.491±0.0080</td><td>0.132±0.0022</td><td>0.630±0.0025</td><td>IL, G</td></tr><tr><td>HLR-short</td><td>3</td><td>0.493±0.0079</td><td>0.140±0.0021</td><td>0.611±0.0029</td><td>IL, G, SR</td></tr><tr><td>Ebisu v2</td><td>0</td><td>0.499±0.0078</td><td>0.163±0.0021</td><td>0.605±0.0026</td><td>IL, G</td></tr><tr><td>Anki-SM-2 trainable</td><td>7</td><td>0.513±0.0089</td><td>0.140±0.0024</td><td>0.618±0.0023</td><td>IL, G</td></tr><tr><td>SM-2 trainable</td><td>6</td><td>0.58±0.012</td><td>0.170±0.0028</td><td>0.597±0.0025</td><td>IL, G</td></tr><tr><td>Anki-SM-2</td><td>0</td><td>0.62±0.011</td><td>0.172±0.0026</td><td>0.613±0.0022</td><td>IL, G</td></tr><tr><td>SM-2-short</td><td>0</td><td>0.65±0.015</td><td>0.170±0.0028</td><td>0.590±0.0027</td><td>IL, G, SR</td></tr><tr><td>SM-2</td><td>0</td><td>0.72±0.017</td><td>0.203±0.0030</td><td>0.603±0.0025</td><td>IL, G</td></tr><tr><td>RMSE-BINS-EXPLOIT</td><td>0</td><td>4.61±0.067</td><td>0.0135±0.00028</td><td>0.655±0.0021</td><td>IL, G</td></tr></tbody></table><p data-pid=\"wN3RSoDG\">按复习次数加权的平均值更能代表在有充足数据可供学习时的「最佳情况」性能。由于几乎所有算法在学习数据充裕时表现更佳，因此按复习次数（n(reviews)）加权会使（代表误差的）平均指标值向更低（更优）的方向偏移。</p><p data-pid=\"z0pxar4G\">未加权的平均值则更能反映「一般情况」下的性能。现实中，并非每个用户都拥有数十万条复习记录，因此算法并非总能完全发挥其潜力。</p><h3><b>优越性</b></h3><p data-pid=\"4UJb6BKC\">上文呈现的各项指标或许难以直接解读。为了更清晰地展示各算法间的相对性能，下图显示了算法 A（行）的对数损失值低于算法 B（列）的用户所占百分比。例如，FSRS-6-recency 相较于 Anki SM-2 算法的默认参数版本，在 99.6% 的用户数据集上表现更优，这意味着对于本基准测试中 99.6% 的用户数据集，FSRS-6-recency 能够更准确地估计回忆概率。但请注意，SM-2 算法最初并非为预测概率而设计，其在本基准测试中之所以能预测概率，完全是因为我们为其额外添加了计算公式。</p><p data-pid=\"mIry9yRS\">此图表基于 9,999 个用户数据集生成。为提高图表的可读性，并未包含所有参评算法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pica.zhimg.com/50/v2-a4168029fe23b58e4979bd6cdd4b02c2_720w.jpg?source=2c26e567\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2790\" data-rawheight=\"2829\" data-original-token=\"v2-a4168029fe23b58e4979bd6cdd4b02c2\" data-default-watermark-src=\"https://picx.zhimg.com/50/v2-c9a45d861af68b8624e47b80e978f37f_720w.jpg?source=2c26e567\" class=\"origin_image zh-lightbox-thumb\" width=\"2790\" data-original=\"https://picx.zhimg.com/v2-a4168029fe23b58e4979bd6cdd4b02c2_r.jpg?source=2c26e567\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2790&#39; height=&#39;2829&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2790\" data-rawheight=\"2829\" data-original-token=\"v2-a4168029fe23b58e4979bd6cdd4b02c2\" data-default-watermark-src=\"https://picx.zhimg.com/50/v2-c9a45d861af68b8624e47b80e978f37f_720w.jpg?source=2c26e567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2790\" data-original=\"https://picx.zhimg.com/v2-a4168029fe23b58e4979bd6cdd4b02c2_r.jpg?source=2c26e567\" data-actualsrc=\"https://pica.zhimg.com/50/v2-a4168029fe23b58e4979bd6cdd4b02c2_720w.jpg?source=2c26e567\"/></figure><p data-pid=\"MrbChr_5\">此外，你可以在<a href=\"https://www.zhihu.com/question/plots/Superiority-9999.png\" class=\"internal\" target=\"_blank\">此处</a>找到完整表格。</p><h3><b>统计显著性</b></h3><p data-pid=\"vC8Ke3BA\">下图展示了采用 Wilcoxon 符号秩检验比较任意两种算法间对数损失的效应大小（r 值）：</p><p data-pid=\"Z66u0IN6\">颜色表示：</p><ul><li data-pid=\"EGZzomS9\">红色系表示行算法的性能劣于列算法：<br/></li><ul><li data-pid=\"h1lGMB9a\">深红色：大效应 (r &gt; 0.5)</li><li data-pid=\"GJ33MwuC\">红色：中等效应 (0.5 ≥ r &gt; 0.2)</li><li data-pid=\"72Si4Bcv\">浅红色：小效应 (r ≤ 0.2)</li></ul><li data-pid=\"NyjmhTMj\">绿色系表示行算法的性能优于列算法：<br/></li><ul><li data-pid=\"JyT0ZWJD\">深绿色：大效应 (r &gt; 0.5)</li><li data-pid=\"IIQnJphX\">绿色：中等效应 (0.5 ≥ r &gt; 0.2)</li><li data-pid=\"l6vp1flD\">浅绿色：小效应 (r ≤ 0.2)</li></ul><li data-pid=\"6BjQuiaW\">灰色表示 p 值大于 0.01，意味着我们无法得出哪个算法性能更优的结论。</li></ul><p data-pid=\"pzzMHb1n\">Wilcoxon 检验同时考虑了成对数据差值的正负号和秩次，但未考虑不同用户数据集中复习次数的差异。因此，尽管该检验结果对于定性分析是可靠的，但在解读具体效应大小时应持谨慎态度。</p><p data-pid=\"9yRKEhTX\">为提高图表的可读性，并未包含所有参评算法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/50/v2-39fcb4e4b0daafaf9fa13f16eb95a432_720w.jpg?source=2c26e567\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2764\" data-rawheight=\"2818\" data-original-token=\"v2-39fcb4e4b0daafaf9fa13f16eb95a432\" data-default-watermark-src=\"https://pic1.zhimg.com/50/v2-6d5937ddf070c564ba2367e9a35914c2_720w.jpg?source=2c26e567\" class=\"origin_image zh-lightbox-thumb\" width=\"2764\" data-original=\"https://picx.zhimg.com/v2-39fcb4e4b0daafaf9fa13f16eb95a432_r.jpg?source=2c26e567\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2764&#39; height=&#39;2818&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2764\" data-rawheight=\"2818\" data-original-token=\"v2-39fcb4e4b0daafaf9fa13f16eb95a432\" data-default-watermark-src=\"https://pic1.zhimg.com/50/v2-6d5937ddf070c564ba2367e9a35914c2_720w.jpg?source=2c26e567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2764\" data-original=\"https://picx.zhimg.com/v2-39fcb4e4b0daafaf9fa13f16eb95a432_r.jpg?source=2c26e567\" data-actualsrc=\"https://pic1.zhimg.com/50/v2-39fcb4e4b0daafaf9fa13f16eb95a432_720w.jpg?source=2c26e567\"/></figure><p data-pid=\"NH8oLU3g\">此外，你可以在<a href=\"https://www.zhihu.com/question/plots/Wilcoxon-9999-collections.png\" class=\"internal\" target=\"_blank\">此处</a>找到完整表格。</p><hr/><p data-pid=\"yccLKxzS\">我之前发表的相关论文：</p><a href=\"https://zhuanlan.zhihu.com/p/577383961\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">KDD&#39;22 | 墨墨背单词：基于时序模型与最优控制的记忆算法 [AI+教育] - 知乎</a><a href=\"https://zhuanlan.zhihu.com/p/656714407\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">Thoughts Memo：IEEE TKDE 2023 | 墨墨背单词：通过捕捉记忆动态，优化间隔重复调度</a><p data-pid=\"MjcBX0UV\">相关研究资料：</p><a href=\"https://zhuanlan.zhihu.com/p/561539418\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">Thoughts Memo：间隔重复记忆算法研究资源汇总</a><p data-pid=\"Ni4FecI9\">我写的入门文章：</p><a href=\"https://zhuanlan.zhihu.com/p/556020884\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">Thoughts Memo：间隔重复记忆算法：e 天内，从入门到入土。</a><p data-pid=\"DTJkIXYj\">我的科研经历：</p><a href=\"https://zhuanlan.zhihu.com/p/543325359\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">Thoughts Memo：我是如何在本科期间发表顶会论文的？（内含开源代码和数据集）</a><p></p>",
  "content_need_truncated": false,
  "created_time": 1746518919,
  "excerpt": "求求你们来做点新数据集、新任务上的工作吧！（顺带 sell 一下我的 work） 在间隔重复领域对学生记忆的时间序列预测任务项目地址： open-spaced-repetition/srs-benchmark: A benchmark for spaced repetition schedulers/algorithms 引言间隔重复算法（Spaced Repetition Algorithms）是一种计算机程序，旨在帮助人们规划抽认卡（flashcards）的复习安排。优秀的间隔重复算法能助你更高效地记忆。它并非让用户进行一次性的突击记…",
  "force_login_when_click_read_more": false,
  "id": "1903118963275170311",
  "is_jump_native": false,
  "link_card_info": {
    "https://zhuanlan.zhihu.com/p/543325359": "{\"card_type\":\"card_1\",\"display\":{\"title\":\"我是如何在本科期间发表顶会论文的？（内含开源代码和数据集）\",\"card_open_url\":\"https://zhuanlan.zhihu.com/p/543325359\",\"desc\":\"2176 赞同 · 88 评论 \\u003ca class=\\\"tag type_a\\\"\\u003e文章\\u003c/a\\u003e\",\"content\":\"\\u003ca class=\\\"text normal/bold\\\" data-color=\\\"#444444\\\" data-night-color=\\\"#D3D3D3\\\"\\u003eThoughts Memo:\\u003c/a\\u003e 最近我在时间线上刷到 @重剑无锋 所写的《我是如何从头开始写一篇顶级论文的 》一文，作者梳理了他论文的创作历程，对我也颇有启发。我自己本科期间发表了一篇国际顶会（ACM SIGKDD）论文和一篇 CCF A 中文核心期刊（中文信息学报）论文，这里也想记录这一路的机缘巧合和心路历程。KDD 论文地址： A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling | Proceedings of the 28th ACM SIGKDD Confe…\",\"image\":{\"image_url\":\"https://pica.zhimg.com/v2-08134e67de67b70a126b02829d90edcc_r.jpg?source=172ae18b\",\"ratio\":1,\"is_video\":false},\"bg_type\":\"dark\"},\"source\":{\"content_type\":\"ARTICLE\",\"content_id\":\"543325359\"},\"za_data\":{\"attach_info\":\"\"}}",
    "https://zhuanlan.zhihu.com/p/556020884": "{\"card_type\":\"card_1\",\"display\":{\"title\":\"间隔重复记忆算法：e 天内，从入门到入土。\",\"card_open_url\":\"https://zhuanlan.zhihu.com/p/556020884\",\"desc\":\"331 赞同 · 145 评论 \\u003ca class=\\\"tag type_a\\\"\\u003e文章\\u003c/a\\u003e\",\"content\":\"\\u003ca class=\\\"text normal/bold\\\" data-color=\\\"#444444\\\" data-night-color=\\\"#D3D3D3\\\"\\u003eThoughts Memo:\\u003c/a\\u003e 我是叶峻峣，论文 A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 的第一作者，现就职于墨墨科技，主要负责墨墨背单词的记忆算法业务。关于我这篇论文的科研经历，请见： [文章: 我是如何在本科期间发表顶会论文的？（内含开源代码和数据集）] 《间隔重复记忆算法：e 天内，从入门到入土。》改编自…\",\"image\":{\"image_url\":\"https://picx.zhimg.com/v2-d5734ba2a8f7443e5dd8001cb46031fc_r.jpg?source=172ae18b\",\"ratio\":1,\"is_video\":false},\"bg_type\":\"dark\"},\"source\":{\"content_type\":\"ARTICLE\",\"content_id\":\"556020884\"},\"za_data\":{\"attach_info\":\"\"}}",
    "https://zhuanlan.zhihu.com/p/561539418": "{\"card_type\":\"card_1\",\"display\":{\"title\":\"间隔重复记忆算法研究资源汇总\",\"card_open_url\":\"https://zhuanlan.zhihu.com/p/561539418\",\"desc\":\"126 赞同 · 11 评论 \\u003ca class=\\\"tag type_a\\\"\\u003e文章\\u003c/a\\u003e\",\"content\":\"\\u003ca class=\\\"text normal/bold\\\" data-color=\\\"#444444\\\" data-night-color=\\\"#D3D3D3\\\"\\u003eThoughts Memo:\\u003c/a\\u003e 离上次写间隔重复记忆算法研究的入门教程《 间隔重复记忆算法：e 天内，从入门到入土。 》已经过去半个多月了，不知道读者朋友们入门间隔重复记忆算法了吗？这次就主要分享一下我在研究过程[1]中用到的资源和阅读过的文献，帮助这个领域的探索者更快地上手。科普墨墨团队制作的间隔重复记忆模型科普，适合零基础入门： 【墨墨科普】几个公式，拯救你的记忆。_哔哩哔哩_bilibili 综述Gwern 所著的间隔重复综述，涵盖了大部分间隔重…\",\"image\":{\"image_url\":\"https://picx.zhimg.com/v2-508467d2d77b37d7642c84c0a995408a_r.jpg?source=172ae18b\",\"ratio\":1,\"is_video\":false},\"bg_type\":\"dark\"},\"source\":{\"content_type\":\"ARTICLE\",\"content_id\":\"561539418\"},\"za_data\":{\"attach_info\":\"\"}}",
    "https://zhuanlan.zhihu.com/p/577383961": "{\"card_type\":\"card_1\",\"display\":{\"title\":\"KDD'22 | 墨墨背单词：基于时序模型与最优控制的记忆算法 [AI+教育]\",\"card_open_url\":\"https://zhuanlan.zhihu.com/p/577383961\",\"desc\":\"552 赞同 · 97 评论 \\u003ca class=\\\"tag type_a\\\"\\u003e文章\\u003c/a\\u003e\",\"content\":\"\\u003ca class=\\\"text normal/bold\\\" data-color=\\\"#444444\\\" data-night-color=\\\"#D3D3D3\\\"\\u003eThoughts Memo:\\u003c/a\\u003e 大家好，我是叶峻峣，墨墨科技的记忆算法工程师。前三篇文章我分别介绍了我的科研经历 [1]、算法教程[2]和研究资源[3]。今天分享一下记忆算法论文 A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling 的详细介绍，该论文由我和哈工深的苏敬勇老师、墨墨背单词的曹译珑博士在 KDD 2022 上发表，研究了如何在背单词场景下对用户复习规划进行优化的问题。与以往的同类工作不同的是，本文构建了包含…\",\"image\":{\"image_url\":\"https://picx.zhimg.com/v2-337e676d732039a25471c1057c54bfa3_r.jpg?source=172ae18b\",\"ratio\":1,\"is_video\":false},\"bg_type\":\"dark\"},\"source\":{\"content_type\":\"ARTICLE\",\"content_id\":\"577383961\"},\"za_data\":{\"attach_info\":\"\"}}",
    "https://zhuanlan.zhihu.com/p/656714407": "{\"card_type\":\"card_1\",\"display\":{\"title\":\"IEEE TKDE 2023 | 墨墨背单词：通过捕捉记忆动态，优化间隔重复调度\",\"card_open_url\":\"https://zhuanlan.zhihu.com/p/656714407\",\"desc\":\"327 赞同 · 54 评论 \\u003ca class=\\\"tag type_a\\\"\\u003e文章\\u003c/a\\u003e\",\"content\":\"\\u003ca class=\\\"text normal/bold\\\" data-color=\\\"#444444\\\" data-night-color=\\\"#D3D3D3\\\"\\u003eThoughts Memo:\\u003c/a\\u003e 大家好，我是叶峻峣，墨墨科技的记忆算法工程师。今天要分享的这篇论文研究了如何在背单词场景下对用户复习规划进行优化的问题。本来这篇论文在今年 3 月份的时候就已经录用了，不过一直处于 early access 阶段。目前终于正式刊出，所以在知乎分享一下论文的中文版本，让记忆算法的研究者能够更方便地了解该领域的前沿进展。 DOI：https://doi.org/10.1109/TKDE.2023.3251721 开源代码：maimemo/SSP-MMC-Plus: Optimizing Spaced Repetition Schedule …\",\"image\":{\"image_url\":\"https://pica.zhimg.com/v2-5285ed3ff6a616b988d261d5e1ec764f_r.jpg?source=172ae18b\",\"ratio\":1,\"is_video\":false},\"bg_type\":\"dark\"},\"source\":{\"content_type\":\"ARTICLE\",\"content_id\":\"656714407\"},\"za_data\":{\"attach_info\":\"\"}}"
  },
  "question": {
    "created": 1743642498,
    "detail": "",
    "id": "1891054381262169131",
    "question_type": "normal",
    "relationship": {},
    "title": "时间序列预测方向有什么好水文章的领域吗，研一，感觉预测都被做的差不多了，创新点完全想不出来。？",
    "type": "question",
    "updated_time": 1743642498,
    "url": "https://api.zhihu.com/questions/1891054381262169131"
  },
  "relationship": {
    "upvoted_followees": []
  },
  "type": "answer",
  "voteup_count": 35
}